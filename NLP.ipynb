{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a2a826",
   "metadata": {},
   "source": [
    "# Text Data Mining\n",
    "\n",
    "Text data mining can be described as the process of extracting essential data from common language text. All the data that we generate via text messages, documents, emails, files are written in common language text. Text mining is primarily used to draw useful insights or patterns from such data.\n",
    "\n",
    "<img src=\"https://i.imgur.com/mQvsNyf.png\" width=\"400\" height=\"350\" class=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35da21e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Areas of text mining in data mining\n",
    "\n",
    "These are the following area of text mining :\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/Y0Ql6J3.png\" width=\"450\" height=\"450\" class=\"center\">\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **Information Extraction:**<br>\n",
    "The automatic extraction of structured data such as entities, entities relationships, and attributes describing entities from an unstructured source is called information extraction.\n",
    "\n",
    "- **Natural Language Processing:**<br>\n",
    "NLP stands for Natural language processing. Computer software can understand human language as same as it is spoken. NLP is primarily a component of artificial intelligence(AI). The development of the NLP application is difficult because computers generally expect humans to \"Speak\" to them in a programming language that is accurate, clear, and exceptionally structured. Human speech is usually not authentic so that it can depend on many complex variables, including slang, social context, and regional dialects.\n",
    "\n",
    "- **Data Mining:**<br>\n",
    "Data mining refers to the extraction of useful data, hidden patterns from large data sets. Data mining tools can predict behaviors and future trends that allow businesses to make a better data-driven decision. Data mining tools can be used to resolve many business problems that have traditionally been too time-consuming.\n",
    "\n",
    "- **Information Retrieval:**<br>\n",
    "Information retrieval deals with retrieving useful data from data that is stored in our systems. Alternately, as an analogy, we can view search engines that happen on websites such as e-commerce sites or any other sites as part of information retrieval.\n",
    "\n",
    "\n",
    "# Text Mining Process:\n",
    "\n",
    "The text mining process incorporates the following steps to extract the data from the document.<br><br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/sDfE2o4.png\" width=\"500\" height=\"400\">\n",
    "\n",
    "- **Text transformation**<br>\n",
    "A text transformation is a technique that is used to control the capitalization of the text.\n",
    "Here the two major way of document representation is given.\n",
    "\n",
    "    - Bag of words\n",
    "    - Vector Space\n",
    "   \n",
    "- **Text Pre-processing**<br>\n",
    "Pre-processing is a significant task and a critical step in Text Mining, Natural Language Processing (NLP), and information retrieval(IR). In the field of text mining, data pre-processing is used for extracting useful information and knowledge from unstructured text data. Information Retrieval (IR) is a matter of choosing which documents in a collection should be retrieved to fulfill the user's need.\n",
    "\n",
    "- **Feature selection**<br>\n",
    "Feature selection is a significant part of data mining. Feature selection can be defined as the process of reducing the input of processing or finding the essential information sources. The feature selection is also called variable selection.\n",
    "\n",
    "- **Data Mining**<br>\n",
    "Now, in this step, the text mining procedure merges with the conventional process. Classic Data Mining procedures are used in the structural database.\n",
    "\n",
    "- **Evaluate**<br>\n",
    "Afterward, it evaluates the results. Once the result is evaluated, the result abandon.\n",
    "\n",
    "\n",
    "- **Applications**<br>\n",
    "These are the following text mining applications:\n",
    "\n",
    "    - Risk Management\n",
    "    - Customer Care Service\n",
    "    - Business Intelligence\n",
    "    - Social Media Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d53c2",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.\n",
    "\n",
    "## Applications of NLP\n",
    "\n",
    "There are the following applications of NLP -\n",
    "\n",
    "**1. Speech Recognition**<br>\n",
    "Speech recognition is used for converting spoken words into text. It is used in applications, such as mobile, home automation, video recovery, dictating to Microsoft Word, voice biometrics, voice user interface, and so on.<br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/EatZjYU.png\" width=\"300\" height=\"100\" class=\"center\">\n",
    "\n",
    "\n",
    "**2. Spam Detection**<br>\n",
    "Spam detection is used to detect unwanted e-mails getting to a user's inbox.<br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/U31OVdW.png\" width=\"400\" class=\"center\">\n",
    "\n",
    "**3. Sentiment Analysis**<br>\n",
    "Sentiment Analysis is also known as opinion mining. It is used on the web to analyse the attitude, behaviour, and emotional state of the sender. This application is implemented through a combination of NLP (Natural Language Processing) and statistics by assigning the values to the text (positive, negative, or natural), identify the mood of the context (happy, sad, angry, etc.)\n",
    "\n",
    "<img src=\"https://i.imgur.com/AYd74N2.png\" width=\"400\" height=\"500\" class=\"center\">\n",
    "\n",
    "**4. Spelling correction**<br>\n",
    "Microsoft Corporation provides word processor software like MS-word, PowerPoint for the spelling correction.\n",
    "\n",
    "<img src=\"https://i.imgur.com/0Ir3LFQ.png\" width=\"400\" height=\"400\" class=\"center\">\n",
    "\n",
    "**5. Chatbot**<br>\n",
    "Implementing the Chatbot is one of the important applications of NLP. It is used by many companies to provide the customer's chat services.\n",
    "\n",
    "<img src=\"https://i.imgur.com/ixoPd2Z.png\" width=\"300\" height=\"400\" class=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6aca79",
   "metadata": {},
   "source": [
    "## Termonigies in NLP\n",
    "These terms are commonly used in the context of natural language processing (NLP) and text analysis:\n",
    "\n",
    "1. **Corpus:**\n",
    "   - A corpus refers to a large and structured collection of text documents. It can include various types of written or spoken texts, such as books, articles, transcripts, web pages, and more. Corpora (plural of corpus) are used for linguistic analysis, language modeling, and training NLP models.\n",
    "\n",
    "2. **Document:**\n",
    "   - In NLP, a document typically refers to a single unit of text, which can be as short as a sentence or as long as an entire book or article. Documents are the individual pieces of text that are analyzed or processed within a corpus.\n",
    "\n",
    "3. **Vocabulary:**\n",
    "   - Vocabulary, in the context of NLP, refers to the set of all unique words or tokens that appear in a corpus or a specific set of documents. It represents the entire lexicon of the language or text data being analyzed. Building a vocabulary is a common preprocessing step in NLP, and it's used to create word embeddings, feature vectors, or perform various text analysis tasks.\n",
    "\n",
    "4. **Words:**\n",
    "   - Words are the basic units of language, and in NLP, they refer to individual linguistic units that make up sentences and documents. Words can vary in length and can include single words (e.g., \"cat\"), multi-word phrases (e.g., \"New York City\"), or even symbols and punctuation. Analyzing words is a fundamental aspect of NLP, and it involves tasks such as tokenization (splitting text into words), stemming (reducing words to their base or root form), and more.\n",
    "\n",
    "These terms are central to understanding and working with text data in natural language processing, and they form the foundation for various NLP techniques and applications, including text classification, sentiment analysis, machine translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2b52b",
   "metadata": {},
   "source": [
    "## How to build an NLP pipeline\n",
    "\n",
    "There are the following steps to build an NLP pipeline -\n",
    "\n",
    "### Step1: Sentence Segmentation\n",
    "\n",
    "Sentence Segment is the first step for building the NLP pipeline. It breaks the paragraph into separate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk --upgrade --quiet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc0804",
   "metadata": {},
   "source": [
    "#### nltk.download('punkt_tab')\n",
    "nltk.download('punkt_tab') is a Python command used to download the \"punkt\" resource from the Natural Language Toolkit (NLTK) library. NLTK is a popular library used for natural language processing (NLP) tasks in Python. The \"punkt\" resource specifically contains a pre-trained tokenizer model used for tokenizing text into individual words or sentences. By downloading this resource, you can use NLTK's tokenizer functionality in your Python scripts or applications. Tokenization is an essential preprocessing step in many NLP tasks, as it breaks down text into smaller units for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"I don't drink coffee.\n",
    "What a wonderful day!\n",
    "Timon and Simba went for a party.\n",
    "Shalini got internship!\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e053ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccbe39",
   "metadata": {},
   "source": [
    "### Step2: Word Tokenization\n",
    "\n",
    "Word Tokenizer is used to break the sentence into separate words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693777e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e7c4c5-930d-488b-a7b8-12d4d7b70c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44669108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "t = TreebankWordTokenizer()\n",
    "print(t.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4465a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'very' in list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f50a2",
   "metadata": {},
   "source": [
    "### Step4: Stemming\n",
    "\n",
    "Stemming is used to normalize words into its base form or root form. For example, celebrates, celebrated and celebrating, all these words are originated with a single root word \"celebrate.\" The big problem with stemming is that sometimes it produces the root word which may not have any meaning.\n",
    "\n",
    "**For Example,** intelligence, intelligent, and intelligently, all these words are originated with a single root word \"intelligen.\" In English, the word \"intelligen\" do not have any meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b57546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ec7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating','eats','ate','writing','programming','programs',\n",
    "        'history','finally','finalized','hyperparameter','sleeping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e646a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word + '----->'+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ea9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc335c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word + '----->'+lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7393e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg = RegexpStemmer('ing$|un|s$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.stem('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.stem('unhappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer('english',ignore_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word + '----->'+snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00806ff",
   "metadata": {},
   "source": [
    "### Step 5: Lemmatization\n",
    "\n",
    "Lemmatization is quite similar to the Stemming. It is used to group different inflected forms of the word, called Lemma. The main difference between Stemming and lemmatization is that it produces the root word, which has a meaning.\n",
    "\n",
    "**For example:** In lemmatization, the words intelligence, intelligent, and intelligently has a root word intelligent, which has a meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pos :-\n",
    "noun - n\n",
    "verb - v\n",
    "adjective - a\n",
    "adverb - r\n",
    "'''\n",
    "for word in words:\n",
    "    print(word + '----->'+lemma.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4095f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming.stem('accordance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma.lemmatize('accordance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "print(sentences)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words\n",
    "            if word.lower() not in list(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34bbf8-a782-41c7-a54b-2878051f2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus ---> Tokenization ---> Stopwords Removal ---> Stemming  ----> Text to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0acad",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) model\n",
    "\n",
    "The Bag of Words (BoW) model is the simplest form of text representation in numbers. Like the term itself, we can represent a sentence as a bag of words vector (a string of numbers).\n",
    "\n",
    "Let’s recall the three types of movie reviews we saw earlier:\n",
    "\n",
    "**Review 1:** This movie is very scary and long.<br>\n",
    "**Review 2:** This movie is not scary and is slow.<br>\n",
    "**Review 3:** This movie is spooky and good.<br>\n",
    "\n",
    "We will first build a vocabulary from all the unique words in the above three reviews. The vocabulary consists of these 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’.\n",
    "\n",
    "We can now take each of these words and mark their occurrence in the three movie reviews above with 1s and 0s. This will give us 3 vectors for 3 reviews:\n",
    "\n",
    "<img src=\"https://i.imgur.com/Q8e9CXG.png\" width=\"500\" height=\"600\" class=\"center\">\n",
    "\n",
    "Vector of Review 1: [1 1 1 1 1 1 1 0 0 0 0]\n",
    "\n",
    "Vector of Review 2: [1 1 2 0 0 1 1 0 1 0 0]\n",
    "\n",
    "Vector of Review 3: [1 1 1 0 0 0 1 0 0 1 1]\n",
    "\n",
    "And that’s the core idea behind a Bag of Words (BoW) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "paragraph = '''This movie is very scary and long.\n",
    "This movie is not scary and is slow.\n",
    "This movie is spooky and good.'''\n",
    "ps = PorterStemmer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'This is one sentence'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ad014",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['This', 'is', 'one', 'sentence']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cef6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\piyus\\Desktop\\techis-ds-wiki-main\\DS\\Step 3-1 NLP\\01_NLP\\FakeNews.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0dced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:1000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede290b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42676447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e05f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d968363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x = df.label)\n",
    "plt.title('Label Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea85da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.label\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df.copy()\n",
    "messages.reset_index(inplace=True,drop=True)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(len(messages)):\n",
    "    news = re.sub('[^a-zA-Z]',' ',messages['text'][i])\n",
    "    news = news.lower()\n",
    "    news = news.split()\n",
    "    news = [porter.stem(word) for word in news if not word in\n",
    "           stopwords.words('english')]\n",
    "    news = ' '.join(news)\n",
    "    corpus.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ca3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e079843",
   "metadata": {},
   "source": [
    "- `max_features`: This parameter specifies the maximum number of features (i.e., words or n-grams) that will be considered. In this case, it's set to 5000, meaning only the top 5000 most frequent words or n-grams will be considered as features.\n",
    "\n",
    "- `ngram_range`: This parameter specifies the range of n-grams to be extracted. An n-gram is a contiguous sequence of n items from a given sample of text or speech. Here, `ngram_range=(1,3)` indicates that unigrams (single words), bigrams (pairs of consecutive words), and trigrams (triplets of consecutive words) will be considered as features.\n",
    "\n",
    "The `CountVectorizer` is a tool used in natural language processing (NLP) for converting a collection of text documents into a matrix of token counts. It essentially creates a sparse matrix where rows represent documents and columns represent features (words or n-grams), and the values in the matrix represent the frequency of each feature in each document. This matrix can then be used as input for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 5000,ngram_range=(1,3))\n",
    "X_cv = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f969ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = pd.concat([df,pd.DataFrame(X_cv)],axis=1)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.drop(['id', 'title', 'author', 'text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c181285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_cv.drop(['label'],axis=1),\n",
    "                                                df_cv['label'],test_size=0.33,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB(alpha=0.01)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7090ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm,annot=True,fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911499b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55068f",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multipling two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db576b1",
   "metadata": {},
   "source": [
    "Term Frequency(TF) = (Number of reps of word in a sentence)/(Number of words in sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a053e88",
   "metadata": {},
   "source": [
    "Inverse Document Frequency(IDF) = log((number of sentence)/(Number of sentence containing the word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e700b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.concat([df,pd.DataFrame(X)],axis=1)\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf.drop(['id','title','author','text'],axis =1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_tf.drop(['label'],axis=1),\n",
    "                                                df_tf['label'],test_size=0.3,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee706165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nv = MultinomialNB()\n",
    "nv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nv.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb113e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58049b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f766a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm,annot=True,fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08093ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nv = GaussianNB()\n",
    "nv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nv.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm,annot=True,fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783fe01",
   "metadata": {},
   "source": [
    "Naive Bayes is a family of probabilistic classification algorithms that are based on Bayes' theorem with the \"naive\" assumption of independence between features. There are several variants of the Naive Bayes algorithm, each with its own set of assumptions and characteristics. The most common types of Naive Bayes classifiers include:\n",
    "\n",
    "1. Gaussian Naive Bayes:\n",
    "   - Assumes that the continuous features follow a Gaussian (normal) distribution.\n",
    "   - Suitable for continuous data where the values of features are real numbers.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "   - Primarily used for text classification and dealing with discrete data, such as text data where each feature represents the frequency of a term (word) in a document.\n",
    "   - Assumes that features follow a multinomial distribution.\n",
    "   - Commonly used in natural language processing tasks like spam detection and document categorization.\n",
    "\n",
    "3. Bernoulli Naive Bayes:\n",
    "   - Used for binary or Boolean feature data, where each feature represents the presence or absence of a particular attribute.\n",
    "   - Assumes that features follow a Bernoulli distribution.\n",
    "   - Often used in text classification tasks where the presence or absence of words in a document is important (e.g., sentiment analysis).\n",
    "\n",
    "4. Complement Naive Bayes:\n",
    "   - An extension of Multinomial Naive Bayes, designed to address class imbalance in text classification problems.\n",
    "   - It is particularly useful when some classes have very few training examples compared to others.\n",
    "\n",
    "5. Categorical Naive Bayes:\n",
    "   - Suitable for categorical data where features represent categories or labels.\n",
    "   - Assumes that features follow a categorical distribution.\n",
    "   - Used when dealing with data that doesn't have a natural ordering, such as nominal categorical variables.\n",
    "\n",
    "6. Hybrid Naive Bayes:\n",
    "   - In some cases, you may encounter hybrid versions that combine different Naive Bayes models to handle more complex data. These hybrids may mix Gaussian, Multinomial, or other Naive Bayes models as needed for the specific problem.\n",
    "\n",
    "The choice of which Naive Bayes variant to use depends on the nature of your data and the problem you're trying to solve. It's essential to select the model that best aligns with the assumptions and characteristics of your dataset, as this can significantly impact the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3ffef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
